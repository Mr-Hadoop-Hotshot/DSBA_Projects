---
title: "RAndom Forest"
author: "Karthik"
date: "2/15/2020"
output: html_document
---
```{r}
#View(Customer_Details_Treated)
library(caTools)
set.seed(1000) #Input any random number
spl = sample.split(Customer_Details_Treated$Personal.Loan, SplitRatio = 0.7)
Customer_Train = subset(Customer_Details_Treated, spl == T)
dim(Customer_Train)
Customer_Test = subset(Customer_Details_Treated, spl == F)
dim(Customer_Test)

```
We will be using the "Customer_Train" dataset in this Random Forest Analysis.

```{r}

nrow(Customer_Train)
head(Customer_Train)
print(sum(Customer_Train$Personal.Loan=="1")/nrow(Customer_Train))

```
The fraction of total dataset is where personal loan is equal 1 is about 9.6%.


Lets build our first random forest

```{r}
#install.packages("randomForest")
library(randomForest)
#View(Customer_Train)

seed=1000
set.seed(seed)
rndFor = randomForest(Personal.Loan ~ ., data =Customer_Train[,c(-1,-5)],
                      ntree=501, mtry = 10, nodesize = 10,importance=TRUE)
#501 Number of tree. 3 is random IV picked. Node size longer the number bigger the tree.
#importance=TRUE gives the informatoin on how significant each IV are.
print(rndFor)
```
Out of Bag (OOB) error rate is about 1.4%.
This is the primary measure of success return.

When random forest is created there is possibility some rows could be omitted in some trees.
We collect all those trees and calcuate OOB error rate for both 1s & 0s. This is done for every rate.



CONFUSION MATRIX
Every Mis-classification is on the OFF DIAGONAL i.e: 12 and 49.
Every Right classification is on the DIAGONAL i.e: 3152 and 307.
(4+41)/3160 will give the error rate.
Accuracy will be calculated after fine tune of the trees.



```{r}
#print(rndFor$err.rate,10)
head(rndFor$err.rate,10)
tail(rndFor$err.rate,10)

```

The head and tail of error rate prediction shows the OOB error rate, error rate for 0 and error rate for 1.
As the rows get larger and larger it can be seen that OOB error rate coloumn approaches the 1.28% as given in OOB error rate from the classification output.

```{r}
plot(rndFor)
#rndFor$err.rate
#plot(rndFor, main="")
legend("topright", c("OOB", "0", "1"), text.col=1:6, lty=1:3, col=1:3)
title(main="Error Rates Random Forest Customer_Train")
```

The x-axis is number of trees and y-axis is the Error rate. Green graph is the error for predicting target equal 1.
Red graph is error rate for predicting target equal 0.

OBB error rate plot tells the number of trees that any trees more than 101 is not valuable


```{r}
importance (rndFor)
```

The "importance" R function talks about accuracy.
Coloumn 1 - Decreasing accuracy for target predicting variable 0
Coloumn 2 - Decreasing accurcy for target predicting variable 1
Coloumn 3 - Mean Decreasing Accuracy
Coloumn 4 - Mean Decrease Accuracy GINI

Using an original dataset we created a random forest.
Inorder to easure the accuracy of the random forest we require certain technique by evaluating how important each variables are. In other words, we are looking at if one of the variable gets  shuffled up the original order in what percentage our accuracy gets effected. What "importance = TRUE" tells is in Mean Decreasing accuracy after a variable is shuffled. Larger the number more important the varibale is.

Based on the above output, "Education","Income per month" and Family.members" are the highly influencing varibales in predicting the target variable "Personal.Loan". 

The "Mean Decrease Gini" measure indicates the affected rate on the accuracy on the prediction if one of the variables are been removed. According to this measure "Income","Education" and "Family members" variables are most important variable in predicting the potential customers who could be converted into a potential customer for a personal loan scheme.

The "Mean Decreasing Accuracy" and "Mean Decrease Accuracy GINI" are 2 different measuring techniques of accuracy of random forest model.
*----------------------------------------------------------------


Now, let's fine tune our tree by selecting random independent variable.

Now we will "tune" the Random Forest by trying different m values. We will stick with 101 trees 
(odd number of trees are preferable). The returned forest, "tRndFor" is the one corresponding to the best m value.

```{r}
View(Customer_Train)
set.seed(seed)
tRndFor = tuneRF(x =Customer_Train[,c(-1,-5,-10)],y=Customer_Train$Personal.Loan,mtryStart = 8, 
                  ntreeTry = 270,stepFactor = 1.5, improve = 0.001, 
                  trace=TRUE, plot = TRUE,doBest = TRUE,nodesize = 50,importance=TRUE)
  
```
mtrryStart indicates the number of variable and step factor is 1.5. It means first number will be 3 independent random variable.Next vaule would be calculated by R using a stepfactor 1.5 which is 3 x 1.5 = 4.5. It's not possible to have 4.5 independent variable,hence, R will round it upto 4. In the next iteration number of independent variable to be chosen woould ne 4 x 1.5 =6 and it continues. And number of trees to be 101. "Improve" is a measure of success if value is maintained below 0.0001. 


The above tuned random forest plot tells that error rate increases at mtry= 6 & 11. Therefore, it stops at 8.

```{r}
trndFor = randomForest(Personal.Loan ~ ., data =Customer_Train[,c(-1,-5)],
                      ntree=501, mtry = 8, nodesize = 10,importance=TRUE)
```


```{r}
importance(tRndFor)
importance(tRndFor)
```


Now, let's try how well model performs in terms of prediction.
```{r}
Customer_Train$Predict.class = predict(tRndFor, Customer_Train, type="class")
Customer_Train$Prob_1= predict(tRndFor, Customer_Train, type="prob")[,"1"]
head(Customer_Train,20)
```
Based on the above prediction and probability coloumn output for first 10 rows it can be said that, for rows with probability with a low value has prediciton of "ZERO". Evidently, for the 11th row with really high probability value of 100% gives a prediction of 1.

Now let's see how well my model performs with the predicted class coloumn along with the personal loan coloumn.



```{r}
Table_tRnd_ConfM_Train=table(Customer_Train$Personal.Loan, Customer_Train$Predict.class)
Table_tRnd_ConfM_Train
print((Table_tRnd_ConfM_Train[1,2]+Table_tRnd_ConfM_Train[2,1])/nrow(Customer_Train))
```

The error rate of mis classification from the off diagnol to be calculated as 1.26%.

We next find the probability threshold that for the top decile. The choice of what threshold you use is quite subjective and depends on the benfits of having Personal Loan=1 vs the cost of sending out, say mailers, to each customer. Since the threshold for the top decile is lower than 0.5, I decided to use 0.5. 

```{r}
qs=quantile(Customer_Train$Prob_1,prob = seq(0,1,length=11))
print(qs)
```

The above output tells you bottom probability lies at ZERO while the top percentile relies approximately on 0.2%,22% and 100% probability.

```{r}
print(qs[10])
threshold=0.5 
```
And measure what fraction of the top decile actually has a Personal.Loan=1. i.e: 21.62%
```{r}
mean((Customer_Train$Personal.Loan[Customer_Train$Prob_1>threshold])=="1")
```
Based on the output we could say that by sending customers who have threshold value of more than 50% has high chance of responding. Probablity of response is about alomost 98.99%.

From this analysis marketing team will be able to forecast the response rate of customers with certain threshod value. Very impressive tool to preplan the marketting campaign.



Now using the tuned Random Forest from the previous step, and redo our errors and top decile calculations for the previously identified threshold with the test dataset.
```{r}
nrow(Customer_Test)
Customer_Test$Predict.class = predict(tRndFor, Customer_Test, type="class")
Customer_Test$Prob_1 = predict(tRndFor, Customer_Test, type="prob")[,"1"]
head(Customer_Test,10)
```

```{r}
Table_tRnd_ConfM_Test=table(Customer_Test$Personal.Loan, Customer_Test$Predict.class)
print((Table_tRnd_ConfM_Test[1,2]+Table_tRnd_ConfM_Test[2,1])/nrow(Customer_Test))
mean((Customer_Test$Personal.Loan[Customer_Test$Prob_1>threshold])=="1")
```
Error rate at test data was about 1.29%. Now it has gone upto 2.2%. Furthermore, for the same threshold we have success rate of 97.4% which is 1.3% less than before. So it is a good model with the number of node size of 50.


```{r}
View(Customer_Details_Treated)
```


